{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python375jvsc74a57bd0b5c37279f6fa5a9ee033eea02bbb2448065059e6348c860591a40ca0db273708",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def accuracy (y_hat, y):\n",
    "    assert(y_hat.size == y.size)\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    X = np.where(y==y_hat,1,0)\n",
    "    return np.sum(X)/len(X)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a,b\n",
    "from autograd.numpy import exp,log\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np \n",
    "from autograd import grad \n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# def gradient(theta,X,y):\n",
    "#         X = np.array(X)\n",
    "#         y = np.array(y)\n",
    "#         theta = np.array(theta)\n",
    "#         print(\"grad\",theta)\n",
    "#         return (y*(log((1.0/(1.0 + exp(-(X.dot(theta))))))) +  (1 - y)*(log(1 - (1.0/(1.0 + exp(-(X.dot(theta))))))))\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        \n",
    "        self.samantha = 0\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods\n",
    "\n",
    "        pass\n",
    "\n",
    "    def loss(self,w, X, y):\n",
    "        n = float(len(X))\n",
    "        y_pred = np.dot(X, w)\n",
    "        # return -((np.sum(-(y_pred * y) + y*log(1.0 + exp(y_pred))) / n ))\n",
    "        # return -((np.sum(-(y_pred * y) + log(1.0 + exp(y_pred))) / n) + self.samantha*(np.sum(np.abs(w))))\n",
    "        return -((np.sum(-(y_pred * y) + log(1.0 + exp(y_pred))) / n ) + self.samantha*(np.sum(np.dot(w.T,w))))\n",
    "    def sigmoid(self,x):\n",
    "        return (1/(1 + exp(-x)))\n",
    "\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        \n",
    "        self.samantha = 0\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit_regularised(self, X, y,batch_size = None, n_iter=200, lr=0.01, lr_type='constant'):\n",
    "       \n",
    "        if (self.fit_intercept):\n",
    "            bias = pd.DataFrame(pd.Series([1.0 for i in range(len(X))]))\n",
    "            X = pd.concat([bias,X],axis=1)\n",
    "        col_len = len(X.columns)\n",
    "        coef_ = np.zeros(col_len)\n",
    "        for i in range(1,n_iter):\n",
    "            cur =  coef_.copy()\n",
    "            coef_ = cur - lr*((X.T).dot((self.sigmoid(X.dot(cur))) - y))       \n",
    "        self.coef_ = coef_\n",
    "        return coef_\n",
    "        pass\n",
    "\n",
    "    def fit_regularised_autograd(self, X, y, batch_size=None, n_iter=200, lr=0.01, lr_type='constant'):\n",
    "\n",
    "        if (self.fit_intercept):\n",
    "            bias = pd.DataFrame(pd.Series([1.0 for i in range(len(X))]))\n",
    "            X = pd.concat([bias,X],axis=1)\n",
    "        col_len = len(X.columns)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        coef_ = np.zeros(col_len)\n",
    "        gradient = grad(self.loss)\n",
    "        # change according to regularization\n",
    "        for i in range(1,n_iter):\n",
    "            cur =  coef_.copy()\n",
    "            coef_ = cur + lr*(gradient(cur,X,y))       \n",
    "        self.coef_ = coef_\n",
    "        return coef_\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "     \n",
    "        X_ = X.copy()\n",
    "        if (self.fit_intercept):\n",
    "            bias = pd.DataFrame(pd.Series([1.0 for i in range(len(X_))]))\n",
    "            X_ = pd.concat([bias,X_],axis=1) \n",
    "        k = self.sigmoid(np.dot(X_,self.coef_))\n",
    "        p = np.where(k<0.5,0,1)\n",
    "        return pd.Series(p)\n",
    "        pass\n",
    "    def softmax(self,X):\n",
    "        t = []\n",
    "        for i in range(len(self.coef_)):\n",
    "            t.append(exp(np.dot(X,self.coef_[i])))\n",
    "        t = np.array(t)\n",
    "        return t/np.sum(t)\n",
    "\n",
    "    def k_class_predict(self, X,flag = False): \n",
    "        X = np.array(X)\n",
    "        soft=np.vectorize(self.softmax,signature='(n)->(m)')\n",
    "        r = soft(X)\n",
    "        if flag == True:\n",
    "            return r\n",
    "        else:\n",
    "            return np.argmax(r,axis = 1)\n",
    "        pass\n",
    "\n",
    "\n",
    "    def confusion(self,X,y):\n",
    "        k = len(self.coef_)\n",
    "        m = np.zeros((k,k))\n",
    "        y_hat = self.k_class_predict(X,False)\n",
    "        for i in range(len(y_hat)):\n",
    "            m[y_hat[i],y[i]] += 1\n",
    "        return m\n",
    "    def fit_k_class_regularised(self, X, y,batch_size = None, n_iter=200, lr=0.01, lr_type='constant'):\n",
    "       \n",
    "        if batch_size==None:\n",
    "            batch_size=len(X)\n",
    "        self.batch_size=batch_size\n",
    "        self.n_iter = n_iter\n",
    "        k = len(np.unique(y))\n",
    "        n = len(X)\n",
    "        batch_size = len(X)\n",
    "        temp = lr \n",
    "        column_length = len(X.columns)\n",
    "        theta = np.zeros((k,column_length))\n",
    "        self.coef_ = theta\n",
    "        soft = self.k_class_predict(X)\n",
    "        for i in range(1,n_iter):\n",
    "            current =  theta.copy()\n",
    "            for j in range(k):\n",
    "                theta[j] = current[j] + lr*(np.sum(X*(np.tile(np.where(y==j,1,0) - soft[:,j],(len(current[0]),1)).T),axis=0))\n",
    "                   \n",
    "        self.coef_ = theta\n",
    "        pass\n",
    "    def plot(self,X,y):\n",
    "    \n",
    "        b = self.coef_[0]\n",
    "        w1, w2 = self.coef_[1:]\n",
    "        c = -b/w2\n",
    "        m = -w1/w2\n",
    "        xmin, xmax = -3, 3\n",
    "        ymin, ymax = -3, 3\n",
    "        xd = np.array([xmin, xmax])\n",
    "        yd = m*xd + c\n",
    "        plt.figure()\n",
    "        plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "        plt.fill_between(xd, yd, ymin, color='tab:blue', alpha=0.2)\n",
    "        plt.fill_between(xd, yd, ymax, color='tab:orange', alpha=0.2)\n",
    "        plt.scatter(X[y==0][:,0],X[y==0][:,1])\n",
    "        plt.scatter(X[y==1][:,0],X[y==1][:,1])\n",
    "        plt.xlim(xmin, xmax)\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.show()\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimum_lamda(X,y):\n",
    "    LR = LogisticRegression(fit_intercept=True)\n",
    "    alpha = 0.01\n",
    "    maxlamda = 0\n",
    "    maxaccu = 0\n",
    "    for i in range (0,100):\n",
    "        LR.samantha = alpha*i\n",
    "        LR.fit_regularised_autograd(pd.DataFrame(X),pd.Series(y))\n",
    "        y_hat = LR.predict(pd.DataFrame(X))\n",
    "        accu = accuracy(y_hat,y)\n",
    "        if accu >= maxaccu :\n",
    "            maxaccu = accu\n",
    "            maxlamda = alpha*i\n",
    "    return maxlamda           \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\pandas\\core\\frame.py:3076: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.iloc._setitem_with_indexer((slice(None), indexer), value)\n",
      "C:\\Users\\shahi\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\pandas\\core\\frame.py:3041: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_array(key, value)\n",
      "0.02\n",
      "[0.9263157894736842, 0.9789473684210527, 0.9841269841269841]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-29a983e6106a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_k_class_regularised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_class_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a81bba530573>\u001b[0m in \u001b[0;36mfit_k_class_regularised\u001b[1;34m(self, X, y, batch_size, n_iter, lr, lr_type)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtheta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "x = load_breast_cancer(as_frame=True)\n",
    "X = x.data\n",
    "minmax = MinMaxScaler()\n",
    "X[list(X)]= minmax.fit_transform(X)\n",
    "y = x.target\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#for 2b\n",
    "print(optimum_lamda(X,y))\n",
    "k = KFold(n_splits=3)\n",
    "accuracies = []\n",
    "\n",
    "for train,test in k.split(X,y):\n",
    "    batch_size = 5\n",
    "    fit_intercept = True\n",
    "    LR = LogisticRegression(fit_intercept=fit_intercept)\n",
    "    LR.fit_regularised(pd.DataFrame(X[train]), pd.Series(y[train]),batch_size)\n",
    "    y_hat = LR.predict(pd.DataFrame(X[test]))\n",
    "    accuracies.append(accuracy(y_hat,y[test]))\n",
    "print(accuracies)\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 30\n",
    "P = 2\n",
    "X = pd.DataFrame(np.random.randn(N, P))\n",
    "y = pd.Series([0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1])\n",
    "\n",
    "LR = LogisticRegression(fit_intercept = False)\n",
    "LR.fit_k_class_regularised(pd.DataFrame(X),pd.Series(y))\n",
    "y_hat = LR.k_class_predict(pd.DataFrame(X))\n",
    "print(y_hat,y)\n",
    "\n",
    "# LR.plot(np.array(X),np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nets'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-db55f2e1268d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# NETS package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nets'"
     ]
    }
   ],
   "source": [
    "\n",
    "# NETS package\n",
    "import nets\n",
    "from nets.nn.modules import Module\n",
    "from nets.nn.activation import *\n",
    "\n",
    "\n",
    "class DNN(Module):\n",
    "    \"\"\"\n",
    "    Dense Neural Network.\n",
    "    Attributes:\n",
    "        layer_dimensions (list(int)): ``list`` of ``int`` containing all layer dimensions. The dimension at index ``i``\n",
    "            is the value of ``layer_dimensions`` at this index.\n",
    "        hidden_dimensions (list(int)): ``list`` of ``int`` containing only hidden dimensions.\n",
    "        training (bool): Boolean to indicate if we are training or not. This function can namely be\n",
    "            used for inference only, in which case we do not need to store the features\n",
    "            values.\n",
    "        activation_hidden (Activation): activation function used in hidden layers.\n",
    "        activation_output (Activation): activation function used in the last layer.\n",
    "        _parameters (dict): ``dict`` containing values of weights and biases for each layers.\n",
    "            - ``weight_{i}``: contains the weight matrix of layer ``i``\n",
    "            - ``bias_{i}``: contains the bias array of layer ``i``\n",
    "        _cache (dict): ``dict`` with\n",
    "            - the linear combinations Z^[l] = W^[l]a^[l-1] + b^[l] for l in [1, L].\n",
    "            - the activations A^[l] = activation(Z^[l]) for l in [1, L].\n",
    "            We cache them in order to use them when computing gradients in the back propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_dimensions, activation_hidden=ReLU()):\n",
    "        super().__init__()\n",
    "        # Add layers\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.hidden_dimensions = layer_dimensions[1: -1]\n",
    "        # Add activation functions\n",
    "        assert isinstance(activation_hidden,\n",
    "                          Activation), \"unrecognized activation function type\"\n",
    "        self.activation_hidden = activation_hidden\n",
    "        self.activation_output = Softmax(axis=1)\n",
    "        # Add weights and biases\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"Initialize the parameters dictionary.\n",
    "        For Dense Neural Network, the parameters are either weights or biases. They are saved in the dictionary\n",
    "        with the following keys:\n",
    "            - w_{i}: weight matrix at layer i,\n",
    "            - b_{i}: bias vector at layer i.\n",
    "        \"\"\"\n",
    "        # Create the weights and biases\n",
    "        for i in range(1, len(self.layer_dimensions)):\n",
    "            # Initialization from He et al.\n",
    "            mu = 0\n",
    "            var = 2 / self.layer_dimensions[i]\n",
    "            sigma = np.sqrt(var)\n",
    "            weight_shape = (\n",
    "                self.layer_dimensions[i - 1], self.layer_dimensions[i])\n",
    "            weight = np.random.normal(loc=mu, scale=sigma, size=weight_shape)\n",
    "            bias = np.zeros((self.layer_dimensions[i], ))\n",
    "\n",
    "            # Saving in the parameters dict\n",
    "            layer_weight = \"w_\" + str(i)\n",
    "            self._parameters[layer_weight] = weight\n",
    "            layer_b = \"b_\" + str(i)\n",
    "            self._parameters[layer_b] = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"One forward step.\n",
    "        Args:\n",
    "            inputs (numpy.array): float numpy array with shape (n^[0], batch_size). Input image batch.\n",
    "        Returns:\n",
    "            outputs (numpy.array): float numpy array with shape (n^[L], batch_size). The output predictions of the\n",
    "                network, where n^[L] is the number of prediction classes. For each input i in the batch,\n",
    "                output[c, i] gives the probability that input ``i`` belongs to class ``c``.\n",
    "        \"\"\"\n",
    "        depth = len(self.layer_dimensions) - 1\n",
    "        z = inputs\n",
    "        # Add the outputs to the features\n",
    "        if self.training:\n",
    "            layer_a = \"a_0\"\n",
    "            self._cache[layer_a] = z\n",
    "        # 1/ Iterates through the depth of the neural network\n",
    "        for i in range(1, depth + 1):\n",
    "            # 1.1/ Get the weights and biases from the params\n",
    "            layer_w = \"w_\" + str(i)\n",
    "            layer_b = \"b_\" + str(i)\n",
    "            weight = self._parameters[layer_w]\n",
    "            bias = self._parameters[layer_b]\n",
    "            # 1.2/ Compute the outputs\n",
    "            z = np.dot(z, weight) + bias\n",
    "            # Add the outputs to the features\n",
    "            if self.training:\n",
    "                layer_z = \"z_\" + str(i)\n",
    "                self._cache[layer_z] = z\n",
    "            # 2/ Hidden Activation\n",
    "            # The activation only occurs in the hidden layers\n",
    "            if i < depth:\n",
    "                z = self.activation_hidden(z)\n",
    "                # Add the outputs to the features\n",
    "                if self.training:\n",
    "                    layer_a = \"a_\" + str(i)\n",
    "                    self._cache[layer_a] = z\n",
    "        # 3/ Output\n",
    "        outputs = self.activation_output(z)\n",
    "        # Add the outputs to the features\n",
    "        if self.training:\n",
    "            layer_a = \"a_\" + str(depth)\n",
    "            self._cache[layer_a] = outputs\n",
    "\n",
    "        return outputs\n",
    "    def fit(self, X, y, batch_size=None, n_iter = 200, lr=0.01, lr_type='constant'):\n",
    "        self.init_parameters()\n",
    "\n",
    "        # if (self.fit_intercept):\n",
    "        #     bias = pd.DataFrame(pd.Series([1.0 for i in range(len(X))]))\n",
    "        #     X = pd.concat([bias,X],axis=1)\n",
    "        # col_len = len(X.columns)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # coef_ = np.zeros(col_len)\n",
    "        # gradient = grad(self.loss)\n",
    "        # change according to regularization\n",
    "        for i in range(1,n_iter):\n",
    "            for j in range (len(X)):\n",
    "                \n",
    "                y_ = self.forward(X)\n",
    "                self.backward(y_, y)\n",
    "            # cur =  coef_.copy()\n",
    "            # coef_ = cur + lr*(gradient(cur,X,y))\n",
    "                   \n",
    "        # self.coef_ = coef_\n",
    "        # return coef_\n",
    "        pass\n",
    "    def nnpredict(self, X):\n",
    "        arr = []\n",
    "        for i in range (len(X)):\n",
    "\n",
    "\n",
    "\n",
    "            arr = arr.append(self.forward(i))\n",
    "        return arr\n",
    "\n",
    "\n",
    "    def backward(self, outputs, labels):\n",
    "\n",
    "\n",
    "        # Layers & shape\n",
    "        depth = len(self.layer_dimensions) - 1\n",
    "        batch_size, num_classes = outputs.shape\n",
    "        coefficient = 1 / batch_size\n",
    "        # 1/ First case: last layer -> output\n",
    "        layer_a = \"a_\" + str(depth - 1)\n",
    "        a = self._cache[layer_a]\n",
    "        Jz = outputs - labels\n",
    "        # Weights gradients\n",
    "        dw = coefficient * np.dot(a.T, Jz)\n",
    "        db = coefficient * np.sum(Jz, axis=0)\n",
    "        self._grad[\"dw_\" + str(depth)] = dw\n",
    "        self._grad[\"db_\" + str(depth)] = db\n",
    "        # 2/ Second case: inside the layers\n",
    "        for i in range(depth - 1, 0, -1):\n",
    "            # Get the weights and biases\n",
    "            layer_w = \"w_\" + str(i + 1)\n",
    "            layer_a = \"a_\" + str(i - 1)\n",
    "            layer_z = \"z_\" + str(i)\n",
    "            w = self._parameters[layer_w]\n",
    "            a = self._cache[layer_a]\n",
    "            z = self._cache[layer_z]\n",
    "            # Gradients\n",
    "            Jz = self.activation_hidden.backward(z) * np.dot(Jz, w.T)\n",
    "            db = coefficient * np.sum(Jz, axis=0)\n",
    "            dw = coefficient * np.dot(a.T, Jz)\n",
    "            self._grad[\"dw_\" + str(i)] = dw\n",
    "            self._grad[\"db_\" + str(i)] = db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'fit'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cac124ac2be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mLR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "N = 30\n",
    "P = 2\n",
    "X = pd.DataFrame(np.random.randn(N, P))\n",
    "y = pd.Series([0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1])\n",
    "\n",
    "LR = LogisticRegression(fit_intercept = False)\n",
    "LR.fit(pd.DataFrame(X),pd.Series(y))\n",
    "y_hat = LR.nnpredict(pd.DataFrame(X))\n",
    "print(y_hat,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nets'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4984defe3f8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nets'"
     ]
    }
   ],
   "source": [
    "import nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model for the dogs vs cats dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "    #model = vgg1()\n",
    "    #model = vgg2 ()\n",
    "    #model = transfer()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\t# filename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig('vgg1' + '_plot.png')\n",
    "\tpyplot.close()\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\t# prepare iterators\n",
    "\ttrain_it = datagen.flow_from_directory('images/train/',\n",
    "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "\ttest_it = datagen.flow_from_directory('images/test/',\n",
    "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "\t# fit model\n",
    "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  }
 ]
}